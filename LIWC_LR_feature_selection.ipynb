{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIWC_LR_feature_selection",
      "provenance": [],
      "collapsed_sections": [
        "0L2eeS-wZiki",
        "RCSvy3essahT"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alenabozny/context-augmentation/blob/master/LIWC_LR_feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBrePjB5rxXI"
      },
      "source": [
        "**Mount Google Drive to the Notebook. This allows us to load datasets that are copyied to the GD directory.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3MTQPfDHF2S",
        "outputId": "fd110915-05b8-4838-f240-2f32b815504c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkjxvO0sDVJ"
      },
      "source": [
        "**Load the dataset (LIWC features for CRED/NONCRED data)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "mwoht0ACHcgm",
        "outputId": "5b980b58-02d8-4517-ad67-e5bed1c54b4f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "liwc_data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/LIWC_paragrafy.csv\", sep=\";\",decimal=',', header=0)\n",
        "\n",
        "liwc_data.head()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "      <th>Rate</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>ppron</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>adverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>negate</th>\n",
              "      <th>verb</th>\n",
              "      <th>adj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>number</th>\n",
              "      <th>quant</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>negemo</th>\n",
              "      <th>anx</th>\n",
              "      <th>anger</th>\n",
              "      <th>sad</th>\n",
              "      <th>social</th>\n",
              "      <th>family</th>\n",
              "      <th>friend</th>\n",
              "      <th>...</th>\n",
              "      <th>health</th>\n",
              "      <th>sexual</th>\n",
              "      <th>ingest</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>achieve</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focuspast</th>\n",
              "      <th>focuspresent</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>relativ</th>\n",
              "      <th>motion</th>\n",
              "      <th>space</th>\n",
              "      <th>time</th>\n",
              "      <th>work</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>relig</th>\n",
              "      <th>death</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>netspeak</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Statins available in the United States include...</td>\n",
              "      <td>CRED</td>\n",
              "      <td>56</td>\n",
              "      <td>98.29</td>\n",
              "      <td>76.25</td>\n",
              "      <td>2.24</td>\n",
              "      <td>25.77</td>\n",
              "      <td>18.67</td>\n",
              "      <td>33.93</td>\n",
              "      <td>60.71</td>\n",
              "      <td>33.93</td>\n",
              "      <td>7.14</td>\n",
              "      <td>5.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.57</td>\n",
              "      <td>1.79</td>\n",
              "      <td>7.14</td>\n",
              "      <td>14.29</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.93</td>\n",
              "      <td>7.14</td>\n",
              "      <td>5.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.36</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.36</td>\n",
              "      <td>1.79</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.79</td>\n",
              "      <td>8.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>44.64</td>\n",
              "      <td>8.93</td>\n",
              "      <td>10.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>25.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Statins are one of the most common medicines p...</td>\n",
              "      <td>CRED</td>\n",
              "      <td>53</td>\n",
              "      <td>93.26</td>\n",
              "      <td>93.42</td>\n",
              "      <td>5.35</td>\n",
              "      <td>61.55</td>\n",
              "      <td>17.67</td>\n",
              "      <td>13.21</td>\n",
              "      <td>92.45</td>\n",
              "      <td>50.94</td>\n",
              "      <td>15.09</td>\n",
              "      <td>9.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.66</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.77</td>\n",
              "      <td>5.66</td>\n",
              "      <td>7.55</td>\n",
              "      <td>20.75</td>\n",
              "      <td>5.66</td>\n",
              "      <td>3.77</td>\n",
              "      <td>3.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.09</td>\n",
              "      <td>11.32</td>\n",
              "      <td>9.43</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.66</td>\n",
              "      <td>3.77</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.89</td>\n",
              "      <td>11.32</td>\n",
              "      <td>5.66</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.89</td>\n",
              "      <td>11.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.43</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.89</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0</td>\n",
              "      <td>20.75</td>\n",
              "      <td>11.32</td>\n",
              "      <td>7.55</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They work on an enzyme that is used by our bod...</td>\n",
              "      <td>CRED</td>\n",
              "      <td>84</td>\n",
              "      <td>74.10</td>\n",
              "      <td>90.47</td>\n",
              "      <td>6.50</td>\n",
              "      <td>25.77</td>\n",
              "      <td>21.00</td>\n",
              "      <td>17.86</td>\n",
              "      <td>94.05</td>\n",
              "      <td>50.00</td>\n",
              "      <td>13.10</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.19</td>\n",
              "      <td>5.95</td>\n",
              "      <td>5.95</td>\n",
              "      <td>15.48</td>\n",
              "      <td>7.14</td>\n",
              "      <td>3.57</td>\n",
              "      <td>9.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.10</td>\n",
              "      <td>4.76</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.19</td>\n",
              "      <td>4.76</td>\n",
              "      <td>2.38</td>\n",
              "      <td>2.38</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>13.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.19</td>\n",
              "      <td>14.29</td>\n",
              "      <td>5.95</td>\n",
              "      <td>2.38</td>\n",
              "      <td>3.57</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>10.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.52</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.33</td>\n",
              "      <td>1.19</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0</td>\n",
              "      <td>16.67</td>\n",
              "      <td>5.95</td>\n",
              "      <td>8.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yep, that’s right…our bodies make cholesterol,...</td>\n",
              "      <td>CRED</td>\n",
              "      <td>76</td>\n",
              "      <td>64.39</td>\n",
              "      <td>85.38</td>\n",
              "      <td>19.27</td>\n",
              "      <td>25.77</td>\n",
              "      <td>19.00</td>\n",
              "      <td>19.74</td>\n",
              "      <td>93.42</td>\n",
              "      <td>50.00</td>\n",
              "      <td>11.84</td>\n",
              "      <td>5.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.58</td>\n",
              "      <td>5.26</td>\n",
              "      <td>14.47</td>\n",
              "      <td>7.89</td>\n",
              "      <td>5.26</td>\n",
              "      <td>10.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.84</td>\n",
              "      <td>5.26</td>\n",
              "      <td>3.95</td>\n",
              "      <td>2.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.32</td>\n",
              "      <td>5.26</td>\n",
              "      <td>2.63</td>\n",
              "      <td>2.63</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>13.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.32</td>\n",
              "      <td>13.16</td>\n",
              "      <td>5.26</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.95</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.16</td>\n",
              "      <td>1.32</td>\n",
              "      <td>10.53</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.63</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0</td>\n",
              "      <td>18.42</td>\n",
              "      <td>6.58</td>\n",
              "      <td>9.21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>That’s because cholesterol is vital for our su...</td>\n",
              "      <td>CRED</td>\n",
              "      <td>101</td>\n",
              "      <td>92.29</td>\n",
              "      <td>61.69</td>\n",
              "      <td>29.80</td>\n",
              "      <td>1.00</td>\n",
              "      <td>25.25</td>\n",
              "      <td>18.81</td>\n",
              "      <td>83.17</td>\n",
              "      <td>43.56</td>\n",
              "      <td>5.94</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.96</td>\n",
              "      <td>7.92</td>\n",
              "      <td>14.85</td>\n",
              "      <td>4.95</td>\n",
              "      <td>3.96</td>\n",
              "      <td>8.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.93</td>\n",
              "      <td>4.95</td>\n",
              "      <td>2.97</td>\n",
              "      <td>1.98</td>\n",
              "      <td>2.97</td>\n",
              "      <td>0.99</td>\n",
              "      <td>5.94</td>\n",
              "      <td>0.99</td>\n",
              "      <td>4.95</td>\n",
              "      <td>1.98</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>10.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.89</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.99</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.97</td>\n",
              "      <td>0.99</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.86</td>\n",
              "      <td>0.99</td>\n",
              "      <td>8.91</td>\n",
              "      <td>3.96</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>20.79</td>\n",
              "      <td>3.96</td>\n",
              "      <td>3.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.99</td>\n",
              "      <td>5.94</td>\n",
              "      <td>2.97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 95 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Body  Rate  ...  Parenth  OtherP\n",
              "0  Statins available in the United States include...  CRED  ...    25.00    0.00\n",
              "1  Statins are one of the most common medicines p...  CRED  ...     0.00    0.00\n",
              "2  They work on an enzyme that is used by our bod...  CRED  ...     0.00    0.00\n",
              "3  Yep, that’s right…our bodies make cholesterol,...  CRED  ...     0.00    0.00\n",
              "4  That’s because cholesterol is vital for our su...  CRED  ...     5.94    2.97\n",
              "\n",
              "[5 rows x 95 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pIVeBF5OcIY"
      },
      "source": [
        "liwc_data_paragraphs = liwc_data[0:1917:3]\n",
        "# X = liwc_data_np[:, 2:]\n",
        "# Y = liwc_data_np[:,1]\n",
        "\n",
        "def y_to_binary(Y):\n",
        "  def label_to_0_1(lbl):\n",
        "    if lbl=='CRED':\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  Y_mapped = map(label_to_0_1, Y)\n",
        "  return np.array(list(Y_mapped))\n",
        "\n",
        "# Y = y_to_binary(Y)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU0zIrhpYWim",
        "outputId": "f58b8cfa-fd2e-4044-ed6a-c92feaa4ab60"
      },
      "source": [
        "np.shape(liwc_data_paragraphs)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(639, 95)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L2eeS-wZiki"
      },
      "source": [
        "## **Recursive Feature Elimination (on Logistic Regression model)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBgr_ZbIZxsx"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtkDtxFljcOy",
        "outputId": "31a1fdba-e2cb-461f-fbc7-acad637e2bd0"
      },
      "source": [
        "def get_n_most_important(num_features):\n",
        "  model = LogisticRegression(max_iter=10000)\n",
        "  rfe = RFE(model, num_features, verbose=0)\n",
        "  fit = rfe.fit(X, Y)\n",
        "\n",
        "  # print(\"Num Features: %s\" % (fit.n_features_))\n",
        "  # print(\"Selected Features: %s\" % (fit.support_))\n",
        "  # print(\"Feature Ranking: %s\" % (fit.ranking_))\n",
        "\n",
        "  features = liwc_data.columns.values[2:]\n",
        "  important_features = []\n",
        "\n",
        "  for f in list(zip(features, fit.support_)):\n",
        "    if f[1]:\n",
        "      important_features.append(f[0])\n",
        "\n",
        "  return important_features\n",
        "\n",
        "important_features = get_n_most_important(50)\n",
        "print(important_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['function', 'pronoun', 'i', 'we', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'conj', 'compare', 'interrog', 'affect', 'posemo', 'negemo', 'anx', 'sad', 'family', 'friend', 'female', 'male', 'cause', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'sexual', 'drives', 'affiliation', 'power', 'reward', 'risk', 'focusfuture', 'leisure', 'home', 'money', 'informal', 'swear', 'assent', 'nonflu', 'Period', 'SemiC', 'QMark', 'Exclam', 'Dash', 'Apostro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCSvy3essahT"
      },
      "source": [
        "## **Build and test LR model on the reduced dataset (only the most important features)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "DdpbmmGBrXhU",
        "outputId": "c3efca68-36d3-4e68-bf22-4914fcfb4ba1"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "shuffle(liwc_data[important_features]).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>negemo</th>\n",
              "      <th>anx</th>\n",
              "      <th>sad</th>\n",
              "      <th>family</th>\n",
              "      <th>friend</th>\n",
              "      <th>female</th>\n",
              "      <th>male</th>\n",
              "      <th>cause</th>\n",
              "      <th>certain</th>\n",
              "      <th>differ</th>\n",
              "      <th>percept</th>\n",
              "      <th>see</th>\n",
              "      <th>hear</th>\n",
              "      <th>feel</th>\n",
              "      <th>sexual</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>Period</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Apostro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>54.76</td>\n",
              "      <td>9.52</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.38</td>\n",
              "      <td>7.14</td>\n",
              "      <td>9.52</td>\n",
              "      <td>7.14</td>\n",
              "      <td>11.90</td>\n",
              "      <td>7.14</td>\n",
              "      <td>4.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.14</td>\n",
              "      <td>2.38</td>\n",
              "      <td>4.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.76</td>\n",
              "      <td>14.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>42.31</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.62</td>\n",
              "      <td>13.46</td>\n",
              "      <td>7.69</td>\n",
              "      <td>3.85</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.85</td>\n",
              "      <td>1.92</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.92</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>48.57</td>\n",
              "      <td>22.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.43</td>\n",
              "      <td>0.00</td>\n",
              "      <td>17.14</td>\n",
              "      <td>2.86</td>\n",
              "      <td>5.71</td>\n",
              "      <td>5.71</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>44.83</td>\n",
              "      <td>8.05</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>8.05</td>\n",
              "      <td>14.94</td>\n",
              "      <td>3.45</td>\n",
              "      <td>4.60</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>4.60</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.05</td>\n",
              "      <td>1.15</td>\n",
              "      <td>5.75</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>45.24</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>11.90</td>\n",
              "      <td>11.90</td>\n",
              "      <td>9.52</td>\n",
              "      <td>4.76</td>\n",
              "      <td>4.76</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "      <td>2.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      function  pronoun     i   we    you  ...  SemiC  QMark  Exclam  Dash  Apostro\n",
              "1050     54.76     9.52  0.00  0.0   0.00  ...    0.0    0.0     0.0  0.00     0.00\n",
              "525      42.31     5.77  0.00  0.0   5.77  ...    0.0    0.0     0.0  1.92     0.00\n",
              "1791     48.57    22.86  0.00  0.0  11.43  ...    0.0    0.0     0.0  0.00     0.00\n",
              "279      44.83     8.05  3.45  0.0   0.00  ...    0.0    0.0     0.0  3.45     1.15\n",
              "291      45.24     7.14  0.00  0.0   4.76  ...    0.0    0.0     0.0  0.00     2.38\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sknDc9KCrsKO",
        "outputId": "950ee72a-a8c4-496a-b2fd-608dafd00410"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing as p\n",
        "\n",
        "\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "def build_logit(liwc_data, important_features):\n",
        "  # split the data into a training set and a validation set\n",
        "  min_max_scaler = p.MinMaxScaler()\n",
        "  liwc_shuffled = shuffle(liwc_data)\n",
        "  X = liwc_shuffled[important_features].values[:, 2:]\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  Y = liwc_shuffled.values[:,1]\n",
        "  Y = y_to_binary(Y)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=VALIDATION_SPLIT)\n",
        "  logreg = LogisticRegression()\n",
        "  logreg.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = logreg.predict(X_test)\n",
        "  print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
        "  print('F1: {:.2f}'.format(metrics.f1_score(y_test, y_pred, average='weighted')))\n",
        "  # return logreg.score(X_test, y_test)\n",
        "  return (metrics.f1_score(y_test, y_pred, average='weighted'), y_test, y_pred)\n",
        "\n",
        "f1, y_test, y_pred = build_logit(liwc_data, important_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of logistic regression classifier on test set: 0.74\n",
            "F1: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am8SbmC5M7J8"
      },
      "source": [
        " **For N iterations check how important feature addition affects the mean accuracy of the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Z6D-YbtUM5C_",
        "outputId": "97859df2-42fc-4fe7-d084-e37fba4af6d3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 90\n",
        "\n",
        "mean_f1 = []\n",
        "\n",
        "for i in range(3, N, 9):\n",
        "  print('Num features: %i' %i)\n",
        "  important_features = get_n_most_important(i)\n",
        "  f1, _, _ = build_logit(liwc_data, important_features)\n",
        "  # mean_accuracies.append(acc)\n",
        "  mean_f1.append(f1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num features: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2c8e6449063f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Num features: %i'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mimportant_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_n_most_important\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliwc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportant_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# mean_accuracies.append(acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4de4a1fcdc01>\u001b[0m in \u001b[0;36mget_n_most_important\u001b[0;34m(num_features)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# print(\"Num Features: %s\" % (fit.n_features_))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Get coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             )\n\u001b[1;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_intercept_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_intercept_dot\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0myz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "w4hF5Se3SV8t",
        "outputId": "e972aae7-ab49-4bc0-bd6c-498a30257d28"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(range(3, N, 9),mean_f1, label='F1 change by number of features in LogisticRegression model')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pO5StreQBvP"
      },
      "source": [
        "**Train on the given category set, then test on the external category.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "431s-Ax7QAyo",
        "outputId": "c94aef5e-0d69-4bfa-d524-f41df681983c"
      },
      "source": [
        "import pickle\n",
        "\n",
        "sent_cat = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/sentences_rates_categories.p')\n",
        "sent_cat[\"body\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     10649\n",
              "unique    10143\n",
              "top           .\n",
              "freq         26\n",
              "Name: body, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmtdRsbZxZfS",
        "outputId": "0ed4553d-7fc5-46ef-9f67-b470760cda9e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# f1, y_test, y_pred = build_logit(liwc_data, get_n_most_important(74))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.48      0.59        48\n",
            "           1       0.74      0.91      0.82        80\n",
            "\n",
            "    accuracy                           0.75       128\n",
            "   macro avg       0.76      0.70      0.70       128\n",
            "weighted avg       0.75      0.75      0.73       128\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vvWJugnx3Ph",
        "outputId": "468ef713-597b-41cb-8ccb-9b11ad7b9cde"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[23 25]\n",
            " [ 7 73]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "2BGke5dH_Fv_",
        "outputId": "eb2b5369-a26c-4b3f-a1c5-f7eaf0af6201"
      },
      "source": [
        "!pip install spacytextblob"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacytextblob\n",
            "  Downloading https://files.pythonhosted.org/packages/56/34/13b9d75a9e3ba30eac115dd0ecbe17d65921a721345b8d915b9ffccc0123/spacytextblob-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: textblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.7/dist-packages (from spacytextblob) (0.15.3)\n",
            "Collecting spacy<3.0.0,>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/89/1539c4024c339650c222b0b2ca2b3e3f13523b7a02671f8001b7b1cee6f2/spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob<0.16.0,>=0.15.3->spacytextblob) (3.2.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (4.41.1)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/92/71ab278f865f7565c37ed6917d0f23342e4f9a0633013113bd435cf0a691/thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (54.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.3.2->spacytextblob) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->spacytextblob) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->spacytextblob) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->spacytextblob) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->spacytextblob) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.3.2->spacytextblob) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->spacytextblob) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.3.2->spacytextblob) (3.7.4.3)\n",
            "Installing collected packages: thinc, spacy, spacytextblob\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.5 spacytextblob-0.1.7 thinc-7.4.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMHiocjs_e0v"
      },
      "source": [
        "# The Model\r\n",
        "Features:\r\n",
        "- TFIDF\r\n",
        "- NER\r\n",
        "- POS\r\n",
        "- LIWC\r\n",
        "- sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADi-UtDH_rFN",
        "outputId": "63e7f606-59b9-4ce0-b737-33aa3ad2ae90"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "# from sklearn.utils import shuffle\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn import preprocessing as p\r\n",
        "\r\n",
        "VALIDATION_SPLIT = 0.1\r\n",
        "\r\n",
        "data = liwc_data_paragraphs\r\n",
        "Y = data['Rate']\r\n",
        "Y = y_to_binary(Y)\r\n",
        "\r\n",
        "np.shape(Y)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(639,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLB2fRE9AkMF"
      },
      "source": [
        "**TFIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T19kOeTZAmmT",
        "outputId": "1ed5cf45-568e-4305-c1bb-05acf66d64c2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk import word_tokenize, sent_tokenize\r\n",
        "\r\n",
        "corpus = data['Body']\r\n",
        "corpus_removed_numbers = [re.sub(r'\\d+', '', s) for s in corpus]\r\n",
        "\r\n",
        "corpus_tokenized = []\r\n",
        "stemmer= PorterStemmer()\r\n",
        "\r\n",
        "for s in corpus_removed_numbers:\r\n",
        "  tokens = word_tokenize(s)\r\n",
        "  stemmed_tokens = [stemmer.stem(word) for word in tokens]\r\n",
        "  corpus_tokenized.append(\" \".join(stemmed_tokens))\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "X_tfidf = vectorizer.fit_transform(corpus_tokenized)\r\n",
        "X_tfidf = np.array(X_tfidf.toarray())\r\n",
        "\r\n",
        "np.shape(X_tfidf)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(639, 3352)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHhdzmScBb8r"
      },
      "source": [
        "**LIWC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfN8WBFMBd7L",
        "outputId": "d0ea32c9-cae0-40cd-9064-c45c169bce50"
      },
      "source": [
        "# important_features = get_n_most_important(50)\r\n",
        "X_liwc = data.values[:, 2:]\r\n",
        "\r\n",
        "np.shape(X_liwc)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(639, 93)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZpNdjMLADfT"
      },
      "source": [
        "**Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_guB3F6ACo4",
        "outputId": "ab56695e-8897-4b76-8ca3-497f64b3a557"
      },
      "source": [
        "import spacy\r\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "spacy_text_blob = SpacyTextBlob()\r\n",
        "nlp.add_pipe(spacy_text_blob)\r\n",
        "\r\n",
        "X_sentiment = []\r\n",
        "for s in corpus:\r\n",
        "  doc = nlp(s)\r\n",
        "  X_sentiment.append([doc._.sentiment.polarity,\r\n",
        "                     doc._.sentiment.subjectivity]\r\n",
        "                     )\r\n",
        "np.shape(X_sentiment)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(639, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CZN5Pq3I5cd",
        "outputId": "b97cc5ed-1c06-44da-e93b-e8c1cd7f4329"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X = np.concatenate((X_tfidf, X_liwc, X_sentiment), axis=1)\n",
        "X = min_max_scaler.fit_transform(X)\n",
        "\n",
        "def eval_and_print_metrics(clf, X, Y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=VALIDATION_SPLIT)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(\"F1 score on test set: \"\n",
        "          \"%0.3f\" % metrics.f1_score(y_test, y_pred, average='weighted'))\n",
        "    print(\"-\" * 10)\n",
        "    return metrics.f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "clf = MLPClassifier(max_iter=2000, solver='adam', hidden_layer_sizes=(50,20)).fit(X_train, y_train)\n",
        "\n",
        "f1s = []\n",
        "for i in range(0,20):\n",
        "  f1s.append(eval_and_print_metrics(clf, X, Y))\n",
        "\n",
        "print(\"Mean weighted f1: \", np.mean(f1s), \" STD: \", np.std(f1s))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on test set: 0.678\n",
            "----------\n",
            "F1 score on test set: 0.602\n",
            "----------\n",
            "F1 score on test set: 0.688\n",
            "----------\n",
            "F1 score on test set: 0.746\n",
            "----------\n",
            "F1 score on test set: 0.660\n",
            "----------\n",
            "F1 score on test set: 0.694\n",
            "----------\n",
            "F1 score on test set: 0.655\n",
            "----------\n",
            "F1 score on test set: 0.636\n",
            "----------\n",
            "F1 score on test set: 0.680\n",
            "----------\n",
            "F1 score on test set: 0.745\n",
            "----------\n",
            "F1 score on test set: 0.736\n",
            "----------\n",
            "F1 score on test set: 0.621\n",
            "----------\n",
            "F1 score on test set: 0.686\n",
            "----------\n",
            "F1 score on test set: 0.650\n",
            "----------\n",
            "F1 score on test set: 0.695\n",
            "----------\n",
            "F1 score on test set: 0.674\n",
            "----------\n",
            "F1 score on test set: 0.644\n",
            "----------\n",
            "F1 score on test set: 0.684\n",
            "----------\n",
            "F1 score on test set: 0.607\n",
            "----------\n",
            "F1 score on test set: 0.748\n",
            "----------\n",
            "Mean weighted f1:  0.676509793645018  STD:  0.04284456048306868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-uJxgmQ5yFOl",
        "outputId": "a74f5242-c63a-4b2f-b629-c002daac1103"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=VALIDATION_SPLIT)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
        "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Log_ROC')\n",
        "plt.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZzNZfvA8c+FsZQ16pcskWxjCU2pVLRIUdG+UMiTSqGoh3ZFj4o2pSRJCw/aRaU8iVSWGY1dkcRYsmSXda7fH/d3xjFmzhxjzvmec+Z6v17nNed81+t8Z+Zc577v733foqoYY4wxOSnkdwDGGGOimyUKY4wxQVmiMMYYE5QlCmOMMUFZojDGGBOUJQpjjDFBWaIweSIii0Skhd9x+E1EhonI4xE+5ygRGRDJc4aLiLQXkW/yuK/9DUaIWD+K2CciK4H/Aw4CO4GvgftUdaefccUbEekE/EtVz/c5jlFAmqo+5nMc/YDTVbVDBM41iih4zwWVlSjix1WqWhJoBDQGHvY5nqMmIkUK4rn9ZNfchMISRZxR1fXAZFzCAEBEzhGRn0Rkq4jMCyyui8gJIvKOiKwVkS0i8lnAuitFJNXb7ycRaRiwbqWIXCoip4jIPyJyQsC6xiKySUQSvNd3iMgS7/iTReTUgG1VRO4VkWXAsuzek4hc7VUzbBWR70WkbpY4HhaRxd7x3xGR4kfxHvqIyHxgl4gUEZG+IvK7iOzwjnmNt21dYBhwrojsFJGt3vLMaiARaSEiaSLSW0Q2iMg6EekccL7yIvKFiGwXkTkiMkBEZuT0uxSR8wN+b6u9Ek2GciIyyYtzlojUCNjvFW/77SKSIiIXBKzrJyIficgHIrId6CQiZ4vIz9551onIayJSNGCfeiLyrYj8LSJ/icgjInI58Ahwk3c95nnblhGRt73jrPHeY2FvXScR+VFEXhKRzUA/b9kMb7146zZ4sS8Qkfoi0hVoD/zbO9cXAb+/S73nhb24Mn53KSJSJadra46Sqtojxh/ASuBS73llYAHwive6ErAZaI37YtDSe32it34SMA4oByQAzb3ljYENQFOgMNDRO0+xbM75HXBnQDyDgGHe87bAcqAuUAR4DPgpYFsFvgVOAEpk895qAbu8uBOAf3vHKxoQx0KgineMH4EBR/EeUr19S3jLbgBO8a7VTd65K3rrOgEzssQ3KuB8LYADwNNerK2B3UA5b/1Y73EckAisznq8gOOeCuwAbvGOVR5oFHDOzcDZ3jUdDYwN2LeDt30RoDewHijuresH7Afaee+xBHAmcI63fTVgCXC/t30pYJ13nOLe66YBx/ogS9yfAm8CxwMnAbOBuwKu3wGgu3euEoHXFGgFpABlAcH9zVTMep1z+Lt/CPd3X9vb9wygvN//m/Hy8D0Ae+TDL9H9w+z0PlgU+B9Q1lvXB3g/y/aTcR+aFYH0jA+yLNu8AfTPsuxXDiWSwH/SfwHfec/F+wC80Hv9FdAl4BiFcB+ep3qvFbg4yHt7HBifZf81QIuAOO4OWN8a+P0o3sMduVzbVKCt9zzzQy1gfeYHGC5R/AMUCVi/AfchXBj3AV07YN2ArMcLWPcw8GkO60YBI7K856VB3sMW4AzveT9gei7v+f6Mc+MS1S85bNePgESBayfbS0DC9/afGnD9VmU5RuY1BS4GfvOuV6GcrnOWv/uMv8FfM35P9sj/h1U9xY92qloK92FVB6jgLT8VuMGrVtjqVZmcj0sSVYC/VXVLNsc7FeidZb8quG/bWX2Mq5KpCFyISz4/BBznlYBj/I1LJpUC9l8d5H2dAvyZ8UJV073tc9r/z4AYQ3kPh51bRG4PqKraCtTn0LUMxWZVPRDwejdQEjgR9y068HzB3ncV4Pcg69dncw4ARORBcVV927z3UIbD30PW91xLRCaKyHqvOuo/AdvnFkegU3Gln3UB1+9NXMki23MHUtXvgNeAocAGERkuIqVDPPfRxGmOkiWKOKOq03DfvgZ7i1bjShRlAx7Hq+qz3roTRKRsNodaDTyTZb/jVPW/2ZxzC/ANrqrmVlw1iAYc564sxymhqj8FHiLIW1qL+wACXD027kNhTcA2gXXRVb19Qn0PmecW13byFnAfrtqiLK5aS0KIMzcbcdUulXOIO6vVQI0g67PltUf8G7gRV1IsC2zj0HuAI9/HG8BSoKaqlsa1PWRsvxo4LYfTZT3OalyJokLA9S6tqvWC7HP4AVWHqOqZuKq5WrgqpVz3I4/Xy4TGEkV8ehloKSJnAB8AV4lIK6/Br7jX6FpZVdfhqoZeF5FyIpIgIhd6x3gLuFtEmnqNjMeLSBsRKZXDOccAtwPXe88zDAMeFpF6kNnYecNRvJfxQBsRuURc43hv3IdRYKK5V0Qqi2tQfxTX5pKX93A87gNpoxdrZ1yJIsNfQOXAht5QqepB4BNcA+5xIlIHd71yMhq4VERuFNfIXl5EGgXZPkMpXELaCBQRkSeA3L6VlwK2Azu9uO4JWDcRqCgi94tIMREpJSJNvXV/AdVEpJD3HtfhvjC8ICKlRaSQiNQQkeYhxI2InOX9rhJwbUN7cKXTjHPllLAARgD9RaSm97tuKCLlQzmvyZ0lijikqhuB94AnVHU1rkH5EdyHx2rct7SM3/1tuLrzpbj69Pu9YyQDd+KqArbgGpA7BTntBKAmsF5V5wXE8inwHDDWq9ZYCFxxFO/lV1zj7KvAJuAq3K3A+wI2G4P7gFqBq34YkJf3oKqLgReAn3EfTA1wjeMZvgMWAetFZFOo7yHAfbhqoPXA+8B/cUkvu1hW4doeeuOq61JxDbS5mYzrR/MbrhpuD8GruAAexJUEd+CSa0aiRVV34G4kuMqLexlwkbf6Q+/nZhGZ6z2/HSgKLMZd849w1ZyhKO2df4sX+2bcjREAbwOJXpXWZ9ns+yLuS8U3uKT3Nq6x3OQD63BnYpq4zob/UtUpfsdytETkOeBkVe3odyzGBGMlCmMiRETqeFUiIiJnA11wt5MaE9WsZ6QxkVMKV910Cq5q6wXgc18jMiYEVvVkjDEmKKt6MsYYE1TMVT1VqFBBq1Wr5ncYxhgTU1JSUjap6ol52TfmEkW1atVITk72OwxjjIkpIvJn7ltlz6qejDHGBGWJwhhjTFCWKIwxxgRlicIYY0xQliiMMcYEZYnCGGNMUGFLFCIy0pv7dmEO60VEhojIchGZLyJNwhWLMcaYvAtniWIUcHmQ9VfghqWuCXTFTZ5ijDEmyoStw52qTheRakE2aQu8582ENlNEyopIRW/yE2OMiRljZq3i89Q1uW8YaaqcnTqNs1KnHdNh/OyZXYnDJ1RJ85YdkShEpCuu1EHVqlUjEpwxxoTq89Q1LF63ncSKoU7xHX4nblpH53EvcOaCn/iz0unHdKyYGMJDVYcDwwGSkpJsuFtjTNRJrFiacXed63cYjiokJcGKX+GFFzi1Rw9ISMjz4fxMFGs4fHL5yt4yY4wxefHTT9CgAZQqBSNGQIUKUKVK7vvlws/bYycAt3t3P50DbLP2CWOMyYPNm+HOO6FZM3jhBbesceN8SRIQxhKFiPwXaAFUEJE04EkgAUBVhwFf4iaPXw7sBjqHKxZjjIlLqvDee/Dgg7BlCzz0kHvks3De9XRLLusVuDdc5zfGmLjXpw8MGgTnnQfDhrlqpzCIicZsY4wxnn/+gV27XPtDly5Qs6b7WSh8LQk2hIcxxsSKr7+G+vXhrrvc69q1XdtEGJMEWKIwxpjot3Yt3HgjXHGFu831vvsienqrejLGmGj2v//BNdfAvn3Qv79rrC5WLKIhWKIwxphotH+/Kz2ccQa0bg0DBsDpx9bDOq+s6skYY6LJ9u3QsydccAEcPOgarceO9S1JgJUojDExImoH3oP8GedJFT76yCWJ9euhWzfYuxeOOy5/gjwGVqIwxsSEjIH3olFixdK0bVQp7wfYuBHatHEN1iefDLNmwWuvRUWSACtRGGNiSFQNvJefSpeGTZvg5Zfh3nuhSHR9NFuJwhhj/DB9OrRqBTt3uruYZs501U5RliTAEoUxxkTWpk3QuTM0bw6//QYrV7rlYe40dyyiNzJjjIknqjBypOtN/cEH8PDDsGiR62kd5aKvjGOMMfHqgw8gMdEN4Fevnt/RhMxKFMYYEy67d8Njj0FaGojAxx/DtGkxlSTAEoUxxoTHl1+6hPDMM/DFF25ZuXJR3RaRE6t6MsZkivtObZGQlgb33+9KD3XruhLEhRf6HdUxib3UZowJm7ju1BYpzzwDkybBf/4DqakxnyTAShTGmCzitlNbOM2eDSVKuBnmBgxwI7yedprfUeUbK1EYY0xebdvmelKfcw48+qhbVr58XCUJsERhjDFHT9WN6FqnjrvVtXt3d+trnLKqJ2OMOVoffAC33w5JSTBxIpx5pt8RhZUlCmOMCcXevbBihbuT6cYb4cABlywKF/Y7srCzqidjjMnN1KluprlWrVzCKFbMjddUAJIEWKIwxpicbdjgSg0XX+ymJh0+POLzVUcDq3oyJoKiuUMbxFCntkhYvhzOPtsNA/7oo+5RooTfUfnCShTGRFA0d2iDGOrUFk7bvd9PjRrQpQvMm+f6RhTQJAFWojAm4qxDW5TatQuefhreegvmz4fKlWHQIL+jigqWKIwx5osv4L77YNUqV4qIkrmqo4UlCmNykZ/tCtYGEGUOHHC3un76qRvp9Ycf4Pzz/Y4q6lgbhTG5yM92BWsDiBKq7meRIlCxIjz7LMyda0kiB1aiMCYE1q4QR2bOdOMzvfUWNGkCQ4f6HVHUsxKFMaZg2LIF7rkHzjsP/vrLvTYhCWuJQkQuB14BCgMjVPXZLOurAu8CZb1t+qrql+GMycS//O6rYO0KcWDcOOjRAzZtcpMKPfUUlCrld1QxI2wlChEpDAwFrgASgVtEJDHLZo8B41W1MXAz8Hq44jEFR373VbB2hTiwdClUqwbJyfDii5YkjlI4SxRnA8tVdQWAiIwF2gKLA7ZRIOOrWhlgbRjjMQWItSkUcHv2wHPPuTaIq66CRx6Bxx4rMGMz5bdwtlFUAlYHvE7zlgXqB3QQkTTgS6B7dgcSka4ikiwiyRs3bgxHrMaYeDFlCjRsCP36ufmqARISLEkcA78bs28BRqlqZaA18L6IHBGTqg5X1SRVTTrxxBMjHqQxJgb89Re0bw8tW7rbX7/5BgYP9juquBDORLEGqBLwurK3LFAXYDyAqv4MFAcqhDEmY0y8+vZb+OgjeOIJWLDAJQyTL8KZKOYANUWkuogUxTVWT8iyzSrgEgARqYtLFFa3ZIwJzbx5LjmAK00sXeruaCpe3N+44kzYEoWqHgDuAyYDS3B3Ny0SkadF5Gpvs97AnSIyD/gv0Ek1o8ukMcbkYOdO6N3bTUHat68bikMEqlf3O7K4FNZ+FF6fiC+zLHsi4PlioFk4YzDGxJnPPoPu3SEtDbp2hYED3VAcJmzs6hpjYseCBXDNNdCggetEd955fkdUIPh915MxxgS3fz9895173qABTJoEKSmWJCLIEoUxJnr99JNrh2jZ0k1NCtC6tesXYSLGEoUxJvr8/bdrf2jWDLZuhU8+gdNP9zuqAsvaKIwx0WXPHmjUCNaudXc29esHJUv6HVWBZonCGBMd0tLcPNXFi0P//i5ZnHGG31EZrOrJGOO3f/5xvalr1HBzVwN07GhJIopYicIY459vvoFu3eD336FDBzj7bL8jMtkIuUQhIseFMxBjTAHTvTu0agWFCrkRX99/H/7v//yOymQj1xKFiJwHjABKAlVF5AzgLlXtFu7gjDFx5uBB97NwYTjnHKhQAfr0sbGZolwoJYqXgFbAZgBVnQdcGM6gjDFxaO5cOPdceN2byLJ9e3jySUsSMSCkqidVXZ1l0cEwxGKMiUc7dsADD8BZZ8GqVVCxot8RmaMUSmP2aq/6SUUkAeiJGw3WGGOC++YbuOMO1yfi7rvhP/+BsmX9jsocpVASxd3AK7hpTNcA3wDWPmHyzZhZq/g8NeucVnm3eN12EiuWzn1DE35Fi8JJJ8HHH0PTpn5HY/IolERRW1XbBy4QkWbAj+EJyRQ0n6euydcP98SKpWnbKOv07CYi9u+HF1+E7dvhmWegRQtITnZ3NpmYFUqieBVoEsIyY/IssWJpxt11rt9hmGMxY4arXlq0CG64AdLTXYKwJBHzckwUInIucB5wooj0ClhVGigc7sCMMTFi82Z3i+vbb0PVqq539ZVX+h2VyUfBUn1RXN+JIkCpgMd24Prwh2aMiQmbN8PYsfDvf8PixZYk4lCOJQpVnQZME5FRqvpnBGMyxkS7JUtg/HjXD6JWLXfb6wkn+B2VCZNQ2ih2i8ggoB6Q2TNGVS8OW1TGmOi0e7drpB40yA393aWLG/HVkkRcC6WVaTSwFKgOPAWsBOaEMSZjTDT6+muoX9/1hbj1Vvj1V5ckTNwLpURRXlXfFpGeAdVRliiMKUh27oTbboPy5WHqVHfbqykwQilR7Pd+rhORNiLSGLBypjHx7uBB+OAD97NkSTfC67x5liQKoFBKFANEpAzQG9d/ojRwf1ijMsb4KyUF7rrL/SxRAq67ziYSKsByLVGo6kRV3aaqC1X1IlU9E/g7ArEZYyJt2zbo0cNNILRmjbvt9dpr/Y7K+CxYh7vCwI24MZ6+VtWFInIl8AhQAmgcmRCNMRFz3XXw3Xdw770wYACUKeN3RCYKBKt6ehuoAswGhojIWiAJ6Kuqn0UiOGNMBKxYASeeCKVKuVtfCxVyQ4Ib4wmWKJKAhqqaLiLFgfVADVXdHJnQjDFhtW8fDB4M/fu76qbnnrMRXk22giWKfaqaDqCqe0RkhSUJY+LE9OluAL8lS+D6612iMCYHwRJFHRGZ7z0XoIb3WgBV1YZhj84Yk/9eegl69YJq1WDSJGjd2u+ITJQLlijqRiwKY0x4pafDrl2uHaJNG9i4ER57DI47zu/ITAwINiigDQRoTDxYtMhVM2XMNFerlhuGw5gQhXVGERG5XER+FZHlItI3h21uFJHFIrJIRMaEMx5jCpTdu+Hhh6FRI9cWceWVoOp3VCYGhdIzO0+8fhhDgZZAGjBHRCao6uKAbWoCDwPNVHWLiJwUrniMKVB++cV1lFu5Ejp3huefhwoV/I7KxKiQShQiUkJEah/lsc8GlqvqClXdB4wF2mbZ5k5gqKpuAVDVDUd5DmNMoIwSQ9Wq7jFtGowcaUnCHJNcSxQichUwGDfjXXURaQQ8rapX57JrJWB1wOs0IOtN2rW8c/yIm161n6p+HWLsxkdjZq3i89Q1+XKsxeu2k1ixdL4cq8A6cABeew0mTIBvv3WjvE6b5ndUJk6EUqLohysdbAVQ1VTc3BT5oQhQE2gB3AK8JSJls24kIl1FJFlEkjdu3JhPpzbH4vPUNSxetz1fjpVYsTRtG1XKl2MVSLNnu7GZHngAiheH7fnzezEmQyhtFPtVdZuIBC4LpUVsDW4IkAyVvWWB0oBZqrof+ENEfsMljsPmu1DV4cBwgKSkJGuNixKJFUsz7q5z/Q6j4Nq5E/r0gTfegIoV4cMP3VhNh/+vGnPMQilRLBKRW4HCIlJTRF4FfgphvzlATRGpLiJFgZuBCVm2+QxXmkBEKuCqolaEGrwxBVpCAnz/PXTvfqiHtSUJEwahJIruuPmy9wJjgG2EMB+Fqh4A7gMmA0uA8aq6SESeFpGM9o3JwGYRWQxMBR6yYUKMCWL5crj9dtixA4oVc/NFvPIKlLY2HhM+orncVy0iTVR1boTiyVVSUpImJyf7HUbMyc/GZzjUAG1VTxGyd6+7xfWZZ6BoUTf0xgUX+B2ViSEikqKqSXnZN5QSxQsiskRE+otI/bycxPgvPxufwRqgI2rqVDe73BNPQLt2sHSpJQkTUbk2ZqvqRSJyMm4SozdFpDQwTlUHhD06k6+sBBCDVF0pYv9++PpraNXK74hMARRShztVXa+qQ4C7gVTgibBGZUxBlp4Ob70Fq1e7xun334eFCy1JGN/kmihEpK6I9BORBUDGHU+Vwx6ZMQXR/Plw/vnQtSuMGOGWVawIJUr4G5cp0ELpRzESGAe0UtW1YY7HmIJp50546ik3V0S5cjBqlLu7yZgoEEobhVVqGxNu/frBCy/Av/4Fzz7rhuAwJkrkmChEZLyq3uhVOQXeQ2sz3BmTH1avdpMJ1akDffu6O5rOP9/vqIw5QrASRU/v55WRCMSYAuPAARgyxN3ueuaZbvC+ChUsSZiolWNjtqqu8552U9U/Ax9At8iEZ0ycmTkTkpKgd29o0QLefdfviIzJVSi3x7bMZtkV+R2IMXFv0iQ47zzYtAk++QS++AKqVfM7KmNyFayN4h5cyeE0EZkfsKoU8GO4AzMmLqjC2rVQqRJceik8/TT07AmlSvkdmTEhC9ZGMQb4ChgIBM53vUNV/w5rVMbEg99+g27d3M/Fi6FkSXjsMb+jMuaoBat6UlVdCdwL7Ah4ICInhD80Y2LUnj3udtcGDSA5GR5+2DrMmZiWW4niSiAFd3ts4ED3CpwWxriMiU3r18OFF8KyZXDLLfDii3DyyX5HZcwxyTFRqOqV3s/8mvbUmPi1f7+bSOj//s8liqFDoWV294EYE3tCGeupmYgc7z3vICIvikjV8IdmTAxIT4dhw6BGDUhLc4P4jRhhScLElVBuj30D2C0iZwC9gd+B98MalTGxYN48d7vrPfdAzZquVGFMHAolURxQNw1eW+A1VR2Ku0XWmIJJFR580PWqXrHCDQM+ZQpUt1paE59CGT12h4g8DNwGXCAihYCE8IZlTBQTgS1boEsXN4BfuXJ+R2RMWIVSorgJ2AvcoarrcXNRDAprVMZEmz//dIP2zfWmj3/rLXjzTUsSpkDINVF4yWE0UEZErgT2qOp7YY/MmGiwfz88/zwkJsK338Kvv7rlhUKaHNKYuBDKXU83ArOBG3DzZs8SkevDHZgxvvvpJ2jSBPr0cXcxLVni+kYYU8CE0kbxKHCWqm4AEJETgSnAR+EMzBjfTZkC27bBZ59B27Z+R2OMb0IpPxfKSBKezSHuZ0xsUYX33oOvvnKv+/RxYzRZkjAFXCgf+F+LyGQR6SQinYBJwJfhDcuYCFu6FC6+GDp2hHfeccuKFXMD+RlTwIXSmP0Q8CbQ0HsMV9U+4Q7MmIj45x94/HFo2BBSU92dTGPH+h2VMVEl2HwUNYHBQA1gAfCgqq6JVGDGRMQXX8CAAdChAwwe7MZqMsYcJliJYiQwEbgON4LsqxGJyJhwW78evv7aPb/hBpg1y/WutiRhTLaC3fVUSlXf8p7/KiJzIxGQMWFz8KCrWnr4YShaFFatcvNEnH2235EZE9WCJYriItKYQ/NQlAh8raqWOEzsmDsX7r4b5sxxU5K+/rpNJmRMiIIlinXAiwGv1we8VuDicAVlTL764w9XaqhQAcaMgZtvduM1GWNCEmzioosiGYgx+UoVFixwdzNVr+5ueb3qKihb1u/IjIk51nHOxJ8//oArr4TGjWH+fLfsttssSRiTR2FNFCJyuYj8KiLLRaRvkO2uExEVkaRwxmPi3L59btjvevVg2jR3u2tiot9RGRPzQhnrKU9EpDAwFGgJpAFzRGSCqi7Osl0poCcwK1yxxKIxs1bxeWr+dVtZvG47iRVL59vxos7Bg262uZQUuPZaePllqFLF76iMiQuhjB4r3lzZT3ivq4pIKPcTng0sV9UVqroPGIubJS+r/sBzwJ6jiDvufZ66hsXrtufb8RIrlqZto0r5dryosd27RoULwx13uA50H39sScKYfBRKieJ1IB13l9PTwA7gY+CsXParBKwOeJ0GNA3cQESaAFVUdZKIPJTTgUSkK9AVoGrVqiGEHB8SK5Zm3F3n+h1GdFKFd991U5K+/bYbuK9bN7+jMiYuhdJG0VRV78X7xq+qW4Cix3pib0rVF4HeuW2rqsNVNUlVk0488cRjPbWJdYsXQ4sW0Lkz1KkDNWr4HZExcS2UEsV+r71BIXM+ivQQ9lsDBJb/K3vLMpQC6gPfi7un/WRggohcrarJIRz/mOV3O0B+ivs2hbx6/nl49FEoXRpGjHDJwmabMyasQvkPGwJ8CpwkIs8AM4D/hLDfHKCmiFQXkaLAzcCEjJWquk1VK6hqNVWtBswEIpYkIP/bAfJT3LYp5JWq+3nyydC+vRsWvEsXSxLGRECuJQpVHS0iKcAluOE72qnqkhD2OyAi9wGTgcLASFVdJCJPA8mqOiH4ESLD2gGi3Nq10LMnXHAB9OgBt9/uHsaYiMk1UYhIVWA38EXgMlVdldu+qvolWSY5UtUncti2RW7HMwXIwYNuPKZHH4X9+92tr8YYX4TSRjEJ1z4hQHGgOvArUC+McZmCLDUV/vUv1yfisstcwrAGa2N8E0rVU4PA194trb7dh7hi4y5uevPnfDmWNRhHqW3bXJXTuHFuvggbwM8YXx11z2xVnSsiTXPfMjz+2X8w345lDcZRQhU+/BCWLXNVTc2bw4oVULy435EZYwDRjLtJctpApFfAy0JAE6C8qrYKZ2A5OeHUuvr3n7m2pZtY8fvvcN99bsa5s86CH3+EhAS/ozIm7ohIiqrmaTy9UO4tLBXwKIZrs8huKA5jQrd3LzzzDNSv75LDK6/ATz9ZkjAmCgWtevI62pVS1QcjFI8pKFavhv793RwRL78MlawK0JholWOJQkSKqOpBoFkE4zHxbONGeO019/z0091QHB9+aEnCmCgXrEQxG9cekSoiE4APgV0ZK1X1kzDHZuJFerqbYe7f/4YdO6BlS6hdG047ze/IjDEhCKWNojiwGTd67JXAVd5PY3K3cKG7i+lf/3ITCqWmuiRhjIkZwUoUJ3l3PC3kUIe7DMFvlTIG3Ixzl13mfo4cCZ06WZ8IY2JQsERRGCjJ4QkigyUKk7PvvnOliKJFYfx4NxR4hQp+R2WMyaNgiWKdqj4dsUhM7EtLcwP4ffKJK0F07gznn+93VMaYYxSsjcLqCExoDhxwt7jWrQtffQUDB7qhwI0xcSFYieKSiEVhYtttt8HYsXDFFTB0KFSv7ndExph8lGOiUNW/IxmIiTFbt0KRIn9rjtAAABnlSURBVFCyJNx7L1x3nXtYY7UxccemBzNHR9WVHurWhccfd8vOPx+uv96ShDFxyhKFCd3y5dCqFdxyC1SuDB06+B2RMSYCLFGY0IwZ4wbwmzXLDcMxcyaceabfURljIuCo56MwBcz+/W5E16QkV730/PNwyil+R2WMiSArUZjsbdjg7ma66Sb3ulYt+OADSxLGFECWKMzh0tNh+HA3HtO4cW58poP5N6ugMSb2WNWTOWTFCtdA/fPP0KIFvPGGG37DGFOgWaIwh5Qp4/pHvPuuq3ay212NMVjVk5kwAa691lUvlS/vhgW//XZLEsaYTJYoCqpVq6BdO2jbFn77Ddatc8sL2Z+EMeZw9qlQ0Bw4AIMHu57V33wDzz0Hv/ziOtAZY0w2rI2ioDl4EEaMgIsvhldfhWrV/I7IGBPlrERREGzZAn36uPmqixWDH390bROWJIwxIbBEEc9UYfRod4vrCy/A1Kluefny1lhtjAmZJYp49dtv0LKl6xdRrRokJ8PVV/sdlTEmBlkbRby6/36XHF5/Hbp2hcKF/Y7IGBOjLFHEk2+/ddVMVaq4XtXFisHJJ/sdlTEmxoW16klELheRX0VkuYj0zWZ9LxFZLCLzReR/InJqOOOJW+vXw623wmWXudtdAU491ZKEMSZfhC1RiEhhYChwBZAI3CIiiVk2+wVIUtWGwEfA8+GKJy6lp8OwYa4U8fHH8OSTro+EMcbko3CWKM4GlqvqClXdB4wF2gZuoKpTVXW393ImYL2+jsbAgXDPPW4CofnzoV8/KF7c76iMMXEmnG0UlYDVAa/TgKZBtu8CfJXdChHpCnQFKFmxRn7FF5t27IBNm6B6dbj7bvfzllvsdldjTNhExe2xItIBSAIGZbdeVYerapKqJiUkJEQ2uGihCp9+ComJbjIhVdcf4tZbLUkYY8IqnIliDVAl4HVlb9lhRORS4FHgalXdG8Z4Yteff7o+ENdeCyecAEOGWHIwxkRMOKue5gA1RaQ6LkHcDNwauIGINAbeBC5X1Q1hjCV2/fwzXHqpez54MPTsCUXsrmZjTOSErUShqgeA+4DJwBJgvKouEpGnRSSji/AgoCTwoYikisiEcMUTc7Zvdz+bNIE77oAlS6B3b0sSxpiIE1X1O4ajcsKpdfXvP5f4HUb4bN4Mffu6IcAXLYKSJf2OyBgTB0QkRVWT8rJvVDRmG1zj9HvvuT4R77zjGqytHcIYEwWsHiMabNvmZpv7/ns491zXia5hQ7+jMsYYwBKFv1RdqaF0aahQAYYPhy5dbDpSY0xUsU8kv0ye7Bqq09JcsvjwQ7jzTksSxpioY59KkbZuHdx8M1x+OezeDRvsrmBjTHSzRBFJQ4e6xurPPoOnnnLjMzVp4ndUxhgTlLVRRFJKCjRt6hJGzZp+R2OMMSGxEkU4bd/uZppLSXGvX3/dtU1YkjDGxBBLFOGgCh99BHXrunGZpk1zy4sXt74RxpiYY4kiv/3xB1x5JdxwA5x0khurqVcvv6Myxpg8s0SR30aPhunT4aWXYM4c1yZhjDExzMZ6yg8//AB797pRXvfuhY0bobJN1meMiR421pNfNm1yI7teeCE8/bRbVqyYJQljTFyx22PzQhVGjYKHHnLjNPXpA48/7ndUBcL+/ftJS0tjz549fodiTFQqXrw4lStXJj9nA7VEkRdffulKEs2auQH86tf3O6ICIy0tjVKlSlGtWjXE7iAz5jCqyubNm0lLS6N69er5dlyregrV7t3w44/ueevW8PnnrtHakkRE7dmzh/Lly1uSMCYbIkL58uXzvcRtiSIUX33lEsIVV8DWra4vxNVX2wB+PrEkYUzOwvH/YZ90waxZ4/pDtG7tGqm/+ALKlvU7KmOMiShLFDnZsAESE2HiRBgwAObNg+bN/Y7KRIGS+TA9bXJyMj169Mhx/cqVKxkzZkzI22fVokULateuzRlnnMFZZ51FamrqMcWbnyZMmMCzzz6bL8f6559/aN68OQcPHsyX44XDwIEDOf3006lduzaTJ0/OdhtV5dFHH6VWrVrUrVuXIUOGAPD555/TsGFDGjVqRFJSEjNmzABg48aNXH755RF7D6hqTD3KVa2jYZWWduj5K6+oLl8e3vOZo7J48WK/Q9Djjz8+7OeYOnWqtmnTJs/7N2/eXOfMmaOqqiNHjtRLL700X+I6cOBAvhwnv7z22mv68ssvh7x9enq6Hjx4MIwRHW7RokXasGFD3bNnj65YsUJPO+20bK/hyJEj9bbbbsuM7a+//lJV1R07dmh6erqqqs6bN09r166duU+nTp10xowZ2Z43u/8TIFnz+Llrdz1l2LYNHnsM3nwTZs50w38fxTc4E3lPfbGIxWu35+sxE08pzZNX1Tvq/VJTU7n77rvZvXs3NWrUYOTIkZQrV445c+bQpUsXChUqRMuWLfnqq69YuHAh33//PYMHD2bixIlMmzaNnj17Aq5+efr06fTt25clS5bQqFEjOnbsSOPGjTO337lzJ927dyc5ORkR4cknn+S6667LMbZzzz2XQYMGAbBr1y66d+/OwoUL2b9/P/369aNt27bs3r2bTp06sXDhQmrXrs3atWsZOnQoSUlJlCxZkrvuuospU6YwdOhQVq5cyZAhQ9i3bx9Nmzbl9ddfB6BLly6ZMd1xxx088MADDBkyhGHDhlGkSBESExMZO3Yso0aNIjk5mddee42VK1dyxx13sGnTJk488UTeeecdqlatSqdOnShdujTJycmsX7+e559/nuuvv/6I9zZ69OjMktfOnTtp27YtW7ZsYf/+/QwYMIC2bduycuVKWrVqRdOmTUlJSeHLL79k/PjxjB8/nr1793LNNdfw1FNPAdCuXTtWr17Nnj176NmzJ127dj3qv4VAn3/+OTfffDPFihWjevXqnH766cyePZtzzz33sO3eeOMNxowZQyGv3fOkk04CDi+97tq167D2h3bt2jF69GiaNWt2TDGGwqqeVGH8eDeA39ChcPfdUKOG31GZGHP77bfz3HPPMX/+fBo0aJD5wdO5c2fefPNNUlNTKVy4cLb7Dh48mKFDh5KamsoPP/xAiRIlePbZZ7ngggtITU3lgQceOGz7/v37U6ZMGRYsWMD8+fO5+OKLg8b29ddf065dOwCeeeYZLr74YmbPns3UqVN56KGH2LVrF6+//jrlypVj8eLF9O/fn5SMEY9xH1BNmzZl3rx5lC9fnnHjxvHjjz9mvqfRo0eTmprKmjVrWLhwIQsWLKBz584APPvss/zyyy/Mnz+fYcOGHRFb9+7d6dixI/Pnz6d9+/aHVa+tW7eOGTNmMHHiRPr27XvEvvv27WPFihVUq1YNcP0HPv30U+bOncvUqVPp3bs36o08sWzZMrp168aiRYv49ddfWbZsGbNnzyY1NZWUlBSmT58OwMiRI0lJSSE5OZkhQ4awefPmI877wAMP0KhRoyMe2VWnrVmzhipVqmS+rly5MmvWrDliu99//51x48aRlJTEFVdcwbJlyzLXffrpp9SpU4c2bdowcuTIzOVJSUn88MMPRxwrHAp2iUIVrr3WTSTUpAlMmABJeerhbnyQl2/+4bBt2za2bt1Kc68Nq2PHjtxwww1s3bqVHTt2ZH57vPXWW5k4ceIR+zdr1oxevXrRvn17rr32Wirn0rN/ypQpjB07NvN1uXLlst2uffv27Nu3j507d2a2UXzzzTdMmDCBwYMHA+5241WrVjFjxozMUk39+vVp2LBh5nEKFy6cWWL53//+R0pKCmeddRbg2ghOOukkrrrqKlasWEH37t1p06YNl112GQANGzakffv2tGvXLjNZBfr555/55JNPALjtttv497//nbmuXbt2FCpUiMTERP76668j9t20aRNlA24uUVUeeeQRpk+fTqFChVizZk3mfqeeeirnnHNO5jX45ptvaNy4MeBKIsuWLePCCy9kyJAhfPrppwCsXr2aZcuWUb58+cPO+9JLL2V7vY/F3r17KV68OMnJyXzyySfccccdmUngmmuu4ZprrmH69Ok8/vjjTJkyBXCljrVr1+Z7LNkpmIli/35ISHC3uZ5/Plx8MXTrBjl84zMmnPr27UubNm348ssvadasWY4Nnkdr9OjRnHnmmTz00EN0796dTz75BFXl448/pnbt2iEfp3jx4pmlIVWlY8eODBw48Ijt5s2bx+TJkxk2bBjjx49n5MiRTJo0ienTp/PFF1/wzDPPsGDBgpDPW6xYscznGSWDQCVKlDisv8Do0aPZuHEjKSkpJCQkUK1atcz1xx9//GHHevjhh7nrrrsOO97333/PlClT+PnnnznuuONo0aJFtv0RHnjgAaZOnXrE8ptvvvmIkk+lSpVYvXp15uu0tDQqVap0xL6VK1fm2muvBVxiyCiRBbrwwgtZsWIFmzZtokKFCuzZs4cSJUocsV04FLyqp++/h4YNXYc5gN69oXt3SxImz8qUKUO5cuUyvwG+//77NG/enLJly1KqVClmzZoFcFgpINDvv/9OgwYN6NOnD2eddRZLly6lVKlS7NixI9vtW7ZsydChQzNfb9myJcfYRIT+/fszc+ZMli5dSqtWrXj11VczP3h/+eUXwJVqxo8fD8DixYtz/EC/5JJL+Oijj9jgzfX+999/8+eff7Jp0ybS09O57rrrGDBgAHPnziU9PZ3Vq1dz0UUX8dxzz7Ft2zZ27tx52PHOO++8zOsyevRoLrjgghzfS1blypXj4MGDmR/m27Zt46STTiIhIYGpU6fy559/Zrtfq1atGDlyZGYsa9asYcOGDWzbto1y5cpx3HHHsXTpUmbOnJnt/i+99BKpqalHPLKrHrv66qsZO3Yse/fu5Y8//mDZsmWcffbZR2zXrl27zOQzbdo0atWqBcDy5cszf1dz585l7969mSWc3377jfoR6vBbcEoUGzfCgw/Ce+9B9epQqpTfEZkYtXv37sOqh3r16sW7776b2Zh92mmn8c477wDw9ttvc+edd1KoUCGaN29OmTJljjjeyy+/zNSpUylUqBD16tXjiiuuoFChQhQuXJgzzjiDTp06ZVaTADz22GPce++91K9fn8KFC/Pkk09mfhvNTokSJejduzeDBg3itdde4/7776dhw4akp6dTvXp1Jk6cSLdu3ejYsSOJiYnUqVOHevXqZRtrYmIiAwYM4LLLLiM9PZ2EhASGDh1KiRIl6Ny5M+np6YC7JfTgwYN06NCBbdu2oar06NHjsKoigFdffZXOnTszaNCgzMbso3HZZZcxY8YMLr30Utq3b89VV11FgwYNSEpKok6dOjnus2TJkswqwZIlS/LBBx9w+eWXM2zYMOrWrUvt2rUzq6qORb169bjxxhtJTEykSJEiDB06NLN01rp1a0aMGMEpp5xC3759ad++PS+99BIlS5ZkxIgRAHz88ce89957JCQkUKJECcaNG5fZoD116lTatGlzzDGGJK+3S/n1yNPtsWPGqJYrp5qQoPrII6q7dh39MUxUiIbbY4/Gjh07Mp8PHDhQe/To4WM0OTtw4ID+888/qqq6fPlyrVatmu7du9fnqHKXkpKiHTp08DsMX1xwwQX6999/Z7vObo/NiwMH3BAcw4a5TnTGRMikSZMYOHAgBw4c4NRTT2XUqFF+h5St3bt3c9FFF7F//35Ulddff52iRYv6HVaumjRpwkUXXcTBgwdzvKssHm3cuJFevXrleCNDfovPiYt27YL+/aFqVddInfEebYygmLdkyRLq1q3rdxjGRLXs/k9s4qJAEydCvXrw3HPw229umYgliTgSa19ujImkcPx/xE+iSEtzfSKuugqOP94NAf7yy35HZfJZ8eLF2bx5syULY7Kh6uajKF68eL4eN37aKFasgMmTYeBA6NULYqB+1Ry9ypUrk5aWxsaNG/0OxZiolDHDXX6K7UQxezb8/DP07OnmrV61CrL0ojTxJSEhIV9n7jLG5C6sVU8icrmI/Coiy0XkiN4oIlJMRMZ562eJSLWQDrx1q2ukPuccePFF13gNliSMMSYMwpYoRKQwMBS4AkgEbhGRrPemdgG2qOrpwEvAc7kdt+TubVCnjhvltUcPWLDAtUkYY4wJi3CWKM4GlqvqClXdB4wF2mbZpi3wrvf8I+ASyWUevxM3rYcqVWDOHNdYXbp0vgdujDHmkHC2UVQCVge8TgOa5rSNqh4QkW1AeWBT4EYi0hXIGBh+ryQnL+TMM8MSdIypQJZrVYDZtTjErsUhdi0OCX0kyCxiojFbVYcDwwFEJDmvnUbijV2LQ+xaHGLX4hC7FoeISHJe9w1n1dMaoErA68resmy3EZEiQBngyJlCjDHG+CaciWIOUFNEqotIUeBmYEKWbSYAHb3n1wPfqfWkMsaYqBK2qievzeE+YDJQGBipqotE5GncKIYTgLeB90VkOfA3LpnkZni4Yo5Bdi0OsWtxiF2LQ+xaHJLnaxFzgwIaY4yJrPgZ68kYY0xYWKIwxhgTVNQmirAN/xGDQrgWvURksYjMF5H/icipfsQZCbldi4DtrhMRFZG4vTUylGshIjd6fxuLRGRMpGOMlBD+R6qKyFQR+cX7P2ntR5zhJiIjRWSDiCzMYb2IyBDvOs0XkSYhHTivU+OF84Fr/P4dOA0oCswDErNs0w0Y5j2/GRjnd9w+XouLgOO85/cU5GvhbVcKmA7MBJL8jtvHv4uawC9AOe/1SX7H7eO1GA7c4z1PBFb6HXeYrsWFQBNgYQ7rWwNfAQKcA8wK5bjRWqIIy/AfMSrXa6GqU1V1t/dyJq7PSjwK5e8CoD9u3LA9kQwuwkK5FncCQ1V1C4CqbohwjJESyrVQIGO8nzLA2gjGFzGqOh13B2lO2gLvqTMTKCsiFXM7brQmiuyG/6iU0zaqegDIGP4j3oRyLQJ1wX1jiEe5XguvKF1FVSdFMjAfhPJ3UQuoJSI/ishMEbk8YtFFVijXoh/QQUTSgC+B7pEJLeoc7ecJECNDeJjQiEgHIAlo7ncsfhCRQsCLQCefQ4kWRXDVTy1wpczpItJAVbf6GpU/bgFGqeoLInIurv9WfVVN9zuwWBCtJQob/uOQUK4FInIp8ChwtarujVBskZbbtSgF1Ae+F5GVuDrYCXHaoB3K30UaMEFV96vqH8BvuMQRb0K5Fl2A8QCq+jNQHDdgYEET0udJVtGaKGz4j0NyvRYi0hh4E5ck4rUeGnK5Fqq6TVUrqGo1Va2Ga6+5WlXzPBhaFAvlf+QzXGkCEamAq4paEckgIySUa7EKuARAROriEkVBnE93AnC7d/fTOcA2VV2X205RWfWk4Rv+I+aEeC0GASWBD732/FWqerVvQYdJiNeiQAjxWkwGLhORxcBB4CFVjbtSd4jXojfwlog8gGvY7hSPXyxF5L+4LwcVvPaYJ4EEAFUdhmufaQ0sB3YDnUM6bhxeK2OMMfkoWquejDHGRAlLFMYYY4KyRGGMMSYoSxTGGGOCskRhjDEmKEsUJiqJyEERSQ14VAuy7c58ON8oEfnDO9dcr/fu0R5jhIgkes8fybLup2ON0TtOxnVZKCJfiEjZXLZvFK8jpZrIsdtjTVQSkZ2qWjK/tw1yjFHARFX9SEQuAwarasNjON4xx5TbcUXkXeA3VX0myPadcCPo3pffsZiCw0oUJiaISElvro25IrJARI4YNVZEKorI9IBv3Bd4yy8TkZ+9fT8Ukdw+wKcDp3v79vKOtVBE7veWHS8ik0Rknrf8Jm/59yKSJCLPAiW8OEZ763Z6P8eKSJuAmEeJyPUiUlhEBonIHG+egLtCuCw/4w3oJiJne+/xFxH5SURqe72UnwZu8mK5yYt9pIjM9rbNbvRdYw7n9/jp9rBHdg9cT+JU7/EpbhSB0t66CriepRkl4p3ez97Ao97zwrixnyrgPviP95b3AZ7I5nyjgOu95zcAs4AzgQXA8bie74uAxsB1wFsB+5bxfn6PN/9FRkwB22TEeA3wrve8KG4kzxJAV+Axb3kxIBmonk2cOwPe34fA5d7r0kAR7/mlwMfe807AawH7/wfo4D0vixv/6Xi/f9/2iO5HVA7hYQzwj6o2ynghIgnAf0TkQiAd9036/4D1AfvMAUZ6236mqqki0hw3Uc2P3vAmRXHfxLMzSEQew40B1AU3NtCnqrrLi+ET4ALga+AFEXkOV131w1G8r6+AV0SkGHA5MF1V//GquxqKyPXedmVwA/j9kWX/EiKS6r3/JcC3Adu/KyI1cUNUJORw/suAq0XkQe91caCqdyxjsmWJwsSK9sCJwJmqul/c6LDFAzdQ1eleImkDjBKRF4EtwLeqeksI53hIVT/KeCEil2S3kar+Jm7ei9bAABH5n6o+HcqbUNU9IvI90Aq4CTfJDrgZx7qr6uRcDvGPqjYSkeNwYxvdCwzBTdY0VVWv8Rr+v89hfwGuU9VfQ4nXGLA2ChM7ygAbvCRxEXDEvODi5gr/S1XfAkbgpoScCTQTkYw2h+NFpFaI5/wBaCcix4nI8bhqox9E5BRgt6p+gBuQMbt5h/d7JZvsjMMNxpZROgH3oX9Pxj4iUss7Z7bUzWjYA+gth4bZzxguulPApjtwVXAZJgPdxSteiRt52JigLFGYWDEaSBKRBcDtwNJstmkBzBORX3Df1l9R1Y24D87/ish8XLVTnVBOqKpzcW0Xs3FtFiNU9RegATDbqwJ6EhiQze7DgfkZjdlZfIObXGqKuqk7wSW2xcBcEVmIGzY+aInfi2U+blKe54GB3nsP3G8qkJjRmI0reSR4sS3yXhsTlN0ea4wxJigrURhjjAnKEoUxxpigLFEYY4wJyhKFMcaYoCxRGGOMCcoShTHGmKAsURhjjAnq/wHNHQit6ciXKwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}